\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{float}

\title{Large-scale Optimal Transport}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.


\author{
  Weijie Chen\thanks{Pre-admission 2019 PKU AAIS} \\
  School of Physics\\
  Peking University\\
  1500011335 \\
  \texttt{1500011335@pku.edu.cn} \\
  \And
  Dinghuai Zhang \\
  School of Mathematics\\
  Peking University\\
  1600013525\\
  \texttt{1600013525@pku.edu.cn} \\
}



\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  
\end{abstract}
\begin{large}
\section{Introduction to Optimal Transport}
Optimal transport (OT) problems are an important series of problems considering the minimal cost of transportation, receiving increasing attention from the community of applied mathematics. Its history can be date back to 18th century, the time of the French engineer Gaspard Monge or 1920s when mathematicians were trying to figure out a way to move things efficiently during World War I. Sometimes optimal transport can be seen as some kind of network flow problem, aiming to "transport" under some certain conditions. To put it in another way, given two distributions and a cost function, we aim to find a transport method that minimize a certain kind of cost. A specific definition can be seen in \ref{def} and \ref{Eq:StdLP_admm_primal}.

Over nearly 200 years, theory of optimal transport have not made much progress until some big mathematical breakthroughs in the 1980s and 1990s. Since then the field has flourished and optimal transport theory has found applications in PDEs, geometry, statistics, economics and image processing [5]. However, although it is such a prosperous research field, efficient algorithm to numerically solve continuous or discrete optimal transport problems is absent. 

\section{Problem Statement}
The standard formulation of optimal transport are derived from couplings. [\textbf{Villani2009}] That is, let $ \left(\mathcal{X}, \mu \right)$ and $\left(\mathcal{Y}, \nu \right)$  be two probability spaces, and a probability distribution $\pi$ on $ \mathcal{X} \times \mathcal{Y} $ is called \emph{coupling} if $ proj_{\mathcal{X}} (\pi) = \mu $ and $ proj_{\mathcal{Y}} (\pi)= \nu $. An optimal transport between $ \left(\mathcal{X}, \mu \right)  $ and $ \left(\mathcal{Y}, \nu \right) $, or an optimal coupling, is a coupling minimize

\begin{equation}
\int_{ \mathcal{X} \times \mathcal{Y} } c ( x, y)  d \pi ( x, y ) 
\label{def}
\end{equation}

Optimal transport problems can be categorized according to the discreteness of $\mu$ and $\nu$. In this report, we only consider discrete optimal tranport problems, where the two distributions are distributions of finite weighted points.
A discrete optimal transport problem can be formulated into a linear program as
\begin{equation} \label{Eq:StdLP}
\begin{aligned}
\min_{\pi} & \sum_{i=1}^{m}\sum_{j=1}^{n} c_{ i j } \pi_{ i j }\\
s.t. & \sum_{j=1}^{n}\pi_{ i j } = \mu_i, \forall i\\
& \sum_{i=1}^{m}\pi_{ i j } = \nu_j, \forall j \\
& \pi_{ij} \geq 0,
\end{aligned}
\end{equation}
where $c$ stands for the cost and $s$ for the transportation plan, while $\mu$ and $\nu$ are restrictions. Note that we always suppose $ c \geq 0 $, $ \mu \geq 0 $, $ \nu \geq 0 $ and $ \sum_{i=1}^{m}{\mu_i} = \sum_{j=1}^{n}{\nu_j} = 1 $ implicitly. From realistic background, $c$ is always valued the squared Euclidean distanced or some other norms. Note that there are $ m n $ variables in this formulation, and this leads to intensive computation.

In order to decrease the number of variables, we can derive the dual problem of discrete optimal transport.
\begin{equation} \label{Eq:StdLP_dual}
  \begin{aligned}
  \max_{\lambda,\eta} & \sum_{i=1}^{m}\mu_{i}\lambda_{i}+\sum_{j=1}^{n}\nu_{j}\eta_{j}\\
  s.t. & c_{i j}-\lambda_{i}-\eta_{j}\geq0, \forall i, j
  \end{aligned}
\end{equation}
Although this formulation only has m + n variables, there are still challenges including the recovery of $\pi$ from $\lambda$ and $\eta$ and the great number of constraints.
\section{Algorithms}
\subsection{ADMM for Primal Problem}
We first implement a first order algorithm called \textbf{alternative direction method of multipliers} (ADMM). According to a reformulation of primal problem, 
\begin{equation} \label{Eq:StdLP_admm_primal}
  \begin{aligned}
  \min_{\pi} & \sum_{i=1}^{m}\sum_{j=1}^{n} c_{ i j } \pi_{ i j }+\mathbb{I}_{+}(\hat{\pi})\\
  s.t. & \sum_{j=1}^{n}\pi_{ i j } = \mu_i, \forall i\\
  & \sum_{i=1}^{m}\pi_{ i j } = \nu_j, \forall j \\
  & \pi=\hat{\pi}
  \end{aligned}
\end{equation}
where $\mathbb{I}_{+}$ is indicator of $\mathbb{R}^{m\times n}_{+}$. The augmented Lagrangian can be written as 
\begin{equation} \label{Eq:admm_primal}
  \begin{aligned}
\mathcal{L}_{\rho}(\pi,\hat{\pi},\lambda,\eta,e)=&\sum_{i=1}^{m}\sum_{j=1}^{n} c_{ i j } \pi_{ i j }+\mathbb{I}_{+}(\hat{\pi})\\
&+\sum_{i=1}^{m}\lambda_{i}\left(\mu_i-\sum_{j=1}^{n}\pi_{ i j }\right)+\sum_{j=1}^{n}\eta_{j}\left(\nu_j-\sum_{i=1}^{m}\pi_{ i j }\right)+\sum_{i=1}^{m}\sum_{j=1}^{n}e_{ij}\left(\pi_{ij}-\hat{\pi}_{ij}\right)\\
&+\frac{\rho}{2}\sum_{i=1}^{m}\left(\mu_i-\sum_{j=1}^{n}\pi_{ i j }\right)^{2}+\frac{\rho}{2}\sum_{j=1}^{n}\left(\nu_j-\sum_{i=1}^{m}\pi_{ i j }\right)^{2}+\frac{\rho}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}\left(\pi_{ij}-\hat{\pi}_{ij}\right)^{2}
\end{aligned}
\end{equation}
The minimizer of $\hat{\pi}$ can be written easily as
\begin{equation}
  argmin_{\hat{\pi}}\mathcal{L}_{\rho}(\pi,\hat{\pi},\lambda,\eta,e)=max\left(\pi+\frac{e}{\rho}, 0\right)
\end{equation}

For the minimizer of $\pi$, we can derive the following equation:
\begin{equation}
  \sum _ { k = 1 } ^ { n } \pi _ { i k } + \sum _ { k = 1 } ^ { m } \pi _ { k j } + \pi_ { i j } = \frac { 1 } { \rho } \left(-e _ { i j } + \lambda _ { i } + \eta _ { j } - c _ { i j } \right) + \mu _ { i } + v _ { j } + \hat{ \pi} _ { i j } \equiv r _ { i j }
\end{equation}
It's a linear equation of $\pi_{ij}$ for the given $r_{ij}$,  which can be solved directly.
\begin{equation}
\pi_ { i j } = r _ { i j } - \frac { 1 } { n + 1 } \sum _ { k = 1 } ^ { n } \left( r _ { i k } - \frac { 1 } { m + n + 1 } \sum _ { l = 1 } ^ { m } r _ { l k } \right) - \frac { 1 } { m + 1 } \sum _ { k = 1 } ^ { m } \left( r _ { k j } - \frac { 1 } { m + n + 1 } \sum _ { l = 1 } ^ { n } r _ { k l } \right)
\end{equation}

Then, we can write the explicit form of ADMM algorithm. This algorithm is implemented in \textbf{ADMM$\_$primal.py}.

\begin{algorithm}[H]
  \SetAlgoNoLine
  \caption{Alternating direction method of multipliers for the primal problem} 
  \KwIn{input data $c$, $\mu$, $\nu$, 
  step size$\alpha$, penalty scalar $\rho$ and maximum iteration $N$} 
  \KwOut{solution $\pi$} 
  initializing $k = 0$\\
  $\pi^{(k)},\hat{\pi}^{(k)},e^{(k)},\lambda^{(k)},\eta^{(k)}:=0$\\
  \While{ $k<N$} 
  {  
   $\pi^{(k+1)}:=argmin_{\pi}\mathcal{L}_{\rho}(\pi,\hat{\pi}^{(k)},\lambda^{(k)},\eta^{(k)},e^{(k)})$\\
   $\hat{\pi}^{(k+1)}:=argmin_{\hat{\pi}}\mathcal{L}_{\rho}(\pi^{(k+1)},\hat{\pi},\lambda^{(k)},\eta^{(k)},e^{(k)})$\\
   $\lambda^{(k+1)}:=\lambda^{(k)}+\alpha\rho(\mu-\sum_{j=1}^{n}\pi_{ i j }^{(k+1)})$\\
   $\eta^{(k+1)}:=\eta^{(k)}+\alpha\rho(\nu-\sum_{i=1}^{m}\pi_{ i j }^{(k+1)})$\\
   $e^{(k+1)}:=e^{(k)}+\alpha\rho(\pi^{(k+1)}-\hat{\pi}^{(k+1)})$\\
   $k:= k+1$
  }
  return $\hat{\pi}$
\end{algorithm}

\subsection{ADMM for Dual Problem}
According the reformulation of dual problem,
\begin{equation}
  \begin{aligned}
    \min_{\lambda,\eta} & -\sum_{i=1}^{m}\mu_{i}\lambda_{i}-\sum_{j=1}^{n}\nu_{j}\eta_{j}+\mathbb{I}_{+}(e)\\
    s.t. & c_{i j}-\lambda_{i}-\eta_{j}-e_{ij}=0, \forall i, j
    \end{aligned}
\end{equation} 
we can write down the augmented Lagrangian as
\begin{equation} \label{Eq:admm_dual}
  \begin{aligned}
\mathcal{L}_{\rho}(\lambda,\eta,e,d)=&-\sum_{i=1}^{m}\mu_{i}\lambda_{i}-\sum_{j=1}^{n}\nu_{j}\eta_{j}+\mathbb{I}_{+}(e)\\
&+\sum_{i=1}^{m}\sum_{j=1}^{n}d_{ij}(c_{i j}-\lambda_{i}-\eta_{j}-e_{ij})+\frac{\rho}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}(c_{i j}-\lambda_{i}-\eta_{j}-e_{ij})^{2}
\end{aligned}
\end{equation}
The minimizer of $e$ can be done directly by solving for zero gradient and projection, while
the minimizer of $\lambda$ and $\eta$ can be done by solving for zero gradient.
\begin{equation}
  \begin{aligned}
    argmin_{e_{ij}}\mathcal{L}_{\rho}(\lambda,\eta,e,d)=& max\left(c_{ij}+\frac{d_{ij}}{\rho}-\lambda_{i}-\eta_{j}, 0\right)\\
    argmin_{\lambda_{i}}\mathcal{L}_{\rho}(\lambda,\eta,e,d)=& \frac{1}{n}\left((\mu_{i}+\sum_{j=1}^{n}d_{ij})/\rho+\sum_{j=1}^{n}(c_{ij}-\eta_{j}-e_{ij})\right)\\
    argmin_{\eta_{j}}\mathcal{L}_{\rho}(\lambda,\eta,e,d)=& \frac{1}{m}\left((\nu_{j}+\sum_{i=1}^{m}d_{ij})/\rho+\sum_{i=1}^{m}(c_{ij}-\lambda_{i}-e_{ij})\right)\\
  \end{aligned}
\end{equation}

The algorithm is implemented in \textbf{ADMM$\_$dual.py}.
Solution $\pi$ can be recovered by $\pi=-d$ from KKT conditions.

\begin{algorithm}[H]
  \SetAlgoNoLine
  \caption{Alternating direction method of multipliers for the primal problem} 
  \KwIn{input data $c$, $\mu$, $\nu$, 
  step size$\alpha$, penalty scalar $\rho$ and maximum iteration $N$} 
  \KwOut{solution $\pi$} 
  initializing $k = 0$\\
  $\lambda^{(k)},\eta^{(k)},e^{(k)},d^{(k)}:=0$\\
  \While{ $k<N$} 
  {  
   $\lambda^{(k+1)}_{i}:=argmin_{\lambda_{i}}\mathcal{L}_{\rho}(\lambda,\eta^{(k)},e^{(k)},d^{(k)})$\\
   $\eta^{(k+1)}_{j}:=argmin_{\eta_{j}}\mathcal{L}_{\rho}(\lambda^{(k+1)},\eta,e^{(k)},d^{(k)})$\\
   $e^{(k+1)}_{ij}:=argmin_{e_{ij}}\mathcal{L}_{\rho}(\lambda^{(k+1)},\eta^{(k+1)},e,d^{(k)})$\\
   $d^{(k+1)}_{ij}:=d^{(k)}_{ij}+\alpha\rho(c_{ij}-\lambda_{i}-\eta_{j}-e_{ij})$\\
   $k:= k+1$
  }
  return $\pi=-d$
\end{algorithm}
\subsection{Add Entropy Regularization: Sinkhorn-Knopp Algorithm}
The discrete entropy of a coupling matrix is defined as
\begin{equation}
\mathbf { H } ( \mathbf { P } ) \stackrel { \mathrm { def } } { = } - \sum _ { i , j } \mathbf { P } _ { i , j } \left( \log \left( \mathbf { P } _ { i , j } \right) - 1 \right)
\end{equation}
The function $\mathbf{H}$ is strongly concave, we can see it by computing its 2-order derivatives:
\begin{align}
\frac{\partial ^ { 2 } \mathbf { H } ( P )}{\partial \mathbf{P}_{ij}^2} = - \operatorname { diag } \left( 1 / \mathbf { P } _ { i , j } \right)  
\end{align}
Notice that $\mathbf { P } _ { i , j } \leq 1$, it's obvious that $\mathbf { H } ( \mathbf { P } ) $ has a good property of convexity.

The idea of the entropic regularization of optimal transport is to use $-\mathbf{H}$ as a regularizing function to obtain approximate solutions to the original transport problem:
\begin{equation}
\mathrm { L } _ { \mathrm { C } } ^ { \varepsilon } ( \mathbf { a } , \mathbf { b } ) \stackrel { \mathrm { def } } { = } \min _ { \mathbf { P } \in \mathbf { U } ( \mathbf { a } , \mathbf { b } ) } \langle \mathbf { P } , \mathbf { C } \rangle - \varepsilon \mathbf { H } ( \mathbf { P } )
\label{sinkhorn target}
\end{equation}
(Actually, this can be interpreted as $\text{KL}(\mathbf{P}||\mathbf{K})$.)
With strong convexity of $\mathbf { H  ( P )}$ our new target has a global minima.

One can show that the solution to \ref{sinkhorn target} has the form of 
\begin{equation}
\mathbf { P } _ { i , j } = \mathbf { u } _ { i } \mathbf { K } _ { i , j } \mathbf { v } _ { j }
\end{equation}
where $\mathbf { K } _ { i , j } = e^{-\mathbf{C}_{i,j}/\epsilon}$ by calculating the KKT condition:
Introducing two dual variables $\mathbf { f } \in \mathbb { R } ^ { n } , \mathbf { g } \in \mathbb { R } ^ { n }$ and calculate the lagrangian:
\begin{equation}
\mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) = \langle \mathbf { P } , \mathbf { C } \rangle - \varepsilon \mathbf { H } ( \mathbf { P } ) - \left\langle \mathbf { f } , \mathbf { P } \mathbf { 1 } _ { n } - \mathbf { a } \right\rangle - \left\langle \mathbf { g } , \mathbf { P } ^ { \mathrm { T } } \mathbf{ 1 } _ { n } - \mathbf { b } \right\rangle
\end{equation}
take first order gradient and we get
\begin{align}
\frac { \partial \mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) } { \partial \mathbf { P } _ { i , j } } &= \mathbf { C } _ { i , j } + \varepsilon \log \left( \mathbf { P } _ { i , j } \right) - \mathbf { f } _ { i } - \mathbf { g } _ { j } = 0\\
\Rightarrow\mathbf { P } _ { i , j } &= e ^ { \mathbf { f } _ { i } / \varepsilon } e ^ { - \mathbf { C } _ { i , j } / \varepsilon } e ^ { \mathbf { g } _ { j } / \varepsilon }
\label{solution for P}
\end{align}
Thus we also get
\begin{align}
\mathbf{u} &= e^{\mathbf{f}/\epsilon}\\
\mathbf{v} &= e^{\mathbf{g}/\epsilon}
\end{align}
Based on the constrain that:
\begin{align}
\operatorname { diag } ( \mathbf { u } ) \mathbf { K } \operatorname { diag } ( \mathbf { v } ) \mathbf { 1 } _ { m } &= \mathbf { a }\\
\operatorname { diag } ( \mathbf { v } ) \mathbf { K } ^ { \top } \operatorname { diag } ( \mathbf { u } ) \mathbf { 1 } _ { n } &= \mathbf { b }
\end{align}
or :
\begin{align}
\mathbf { u } \odot ( \mathbf { K } \mathbf { v } ) = \mathbf { a } \quad \text { and } \quad \mathbf { v } \odot \left( \mathbf { K } ^ { \mathrm { T } } \mathbf { u } \right) = \mathbf { b }
\label{solution for marginal}
\end{align}
(where $\odot$ means entry-wise multiplication of vectors) we can develop our algorithm as iteratively updating $\mathbf { u }$ and $\mathbf { v }$:
\begin{align}
\mathbf { u } ^ { ( \ell + 1 ) }  { = } \frac { \mathbf { a } } { \mathbf { K } \mathbf { v } ^ { ( \ell ) } } \text { and } \mathbf { v } ^ { ( \ell + 1 ) } { = } \frac { \mathbf { b } } { \mathbf { K } ^ { \mathrm { T } } \mathbf { u } ^ { ( \ell + 1 ) } }
\end{align}
with $\mathbf { v } ^ { ( 0 ) } = \mathbf { 1 } _ { m }$ and $\mathbf { K } _ { i ,j } = e^{-\mathbf{C}_{i,j}/\epsilon}$.

\subsection{Sinkhorn-Newton Method}
From \ref{solution for P} and \ref{solution for marginal} we can conclude that our target could be reformulated as finding a zero point of

\begin{displaymath}
F(\mathbf { f },\mathbf { g}) :=
\left( \begin{array}{c}
a -  \operatorname { diag } (   e ^ { -\mathbf { f }  / \epsilon } ) \mathbf { K }  e ^ { -\mathbf { g }  / \epsilon } \\
b -  \operatorname { diag } (   e ^ { -\mathbf { g }  / \epsilon } ) \mathbf { K }^ { \top }  e ^ { -\mathbf { f }  / \epsilon }
\end{array} \right)
\end{displaymath}
where $a,b,\epsilon$ and $\mathbf { K}$ are known.
What we need to do is to use newton-raphson method to find its zero points:
\begin{align}
\left( \begin{array} { l } { \mathbf { f }^ { k + 1 } } \\ { \mathbf { g } ^ { k + 1 } } \end{array} \right) = \left( \begin{array} { l } { \mathbf { f } ^ { k } } \\ { \mathbf { g } ^ { k } } \end{array} \right) - J _ { F } \left( \mathbf { f } ^ { k } , \mathbf { g } ^ { k } \right) ^ { - 1 } F \left( \mathbf { f } ^ { k } , \mathbf { g } ^ { k } \right)
\end{align}
where the Jacobian of F is:
\begin{align}
J _ { F } ( \mathbf { f }  ,  \mathbf { g } ) = \frac { 1 } { \varepsilon } \left[ \begin{array} { c c } { \operatorname { Diag } \left( \mathbf{P} \mathbf { 1 } _ { m } \right) } & { \mathbf{P} } \\ { \mathbf{P} ^ { \top } } & { \operatorname { Diag } \left( \mathbf{P} ^ { \top } \mathbf { 1 } _ { n } \right) } \end{array} \right]
\end{align}
that is, we can use conjugate gradient to solve
\begin{align}
J _ { F } \left(  \mathbf {f} ^ { k } ,  \mathbf {g} ^ { k } \right) \left( \begin{array} { c } { \delta  \mathbf {f} } \\ { \delta  \mathbf {g} } \end{array} \right) = - F \left(  \mathbf {f} ^ { k } ,  \mathbf {g} ^ { k } \right)
\end{align}
and then update variables by
\begin{align}
\begin{aligned}  \mathbf {f} ^ { k + 1 } & =  \mathbf {f} ^ { k } + \delta  \mathbf {f} \\  \mathbf {g} ^ { k + 1 } & =  \mathbf {g} ^ { k } + \delta  \mathbf {g} \end{aligned}
\end{align}
Because $ \mathbf {P} ^ { k } : = \operatorname { Diag } \left( \mathrm { e } ^ { -  \mathbf {f} ^ { k } / \varepsilon } \right) \mathbf { K} \operatorname { Diag } \left( \mathrm { e } ^ { -  \mathbf {g} ^ { k } / \varepsilon } \right)$, the update step can be rewrite as
\begin{align}
\begin{aligned}  \mathbf {P} ^ { k + 1 } & = \operatorname { Diag } \left( \mathrm { e } ^ { - \left[  \mathbf {f} ^ { k } + \delta  \mathbf {f} \right] / \varepsilon } \right)  \mathbf {K} \operatorname { Diag } \left( \mathrm { e } ^ { - \left[  \mathbf {g} ^ { k } + \delta  \mathbf {g} \right] / \varepsilon } \right) \\ & = \operatorname { Diag } \left( \mathrm { e } ^ { - \delta  \mathbf {f} / \varepsilon } \right)  \mathbf {P} ^ { k } \operatorname { Diag } \left( \mathrm { e } ^ { - \delta  \mathbf {g} / \varepsilon } \right) \end{aligned}
\end{align}


\begin{algorithm}[H]
  \SetAlgoNoLine
  \caption{Sinkhorn-Newton method in primal variable} 
  \KwIn{$\mathbf{a} \in \Sigma _ { n } , \mathbf{b} \in \Sigma _ { m } , \mathbf{C} \in \mathbb { R } ^ { n \times m }$} 
   initializing $\mathbf{P} ^ { 0 } = \exp ( - \mathbf{C} / \varepsilon ) ,$ set $k = 0$\\
  \Repeat
  { some stopping criteria fulfilled }{
 $\mathbf{a} ^ { k } \gets \mathbf{P} ^ { k } \mathbf{1}_ { m }$\\  
 $\mathbf{b} ^ { k } \gets \left( \mathbf{P} ^ { k } \right) ^ { \top } \mathbf { 1 } _ { n }$\\
  compute $\delta \mathbf{f}, \delta \mathbf{g}$: 
  \quad$\frac { 1 } { \varepsilon } \left[ \begin{array} { c c } { \operatorname { Diag } \left( \mathbf{a} ^ { k } \right) } & { \mathbf{P} ^ { k } } \\ { \left( \mathbf{P} ^ { k } \right) ^ { \top } } & { \operatorname { Diag } \left( \mathbf{b} ^ { k } \right) } \end{array} \right] \left[ \begin{array} { l } { \delta \mathbf{f} } \\ { \delta \mathbf{g} } \end{array} \right] = \left[ \begin{array} { l } { \mathbf{a} ^ { k } - \mathbf{a} } \\ { \mathbf{b} ^ { k } - \mathbf{b} } \end{array} \right]$\\
$\mathbf{P} ^ { k + 1 } \gets \operatorname { Diag } \left( \mathrm { e } ^ { - \delta \mathbf{f} / \varepsilon } \right) \mathbf{P} ^ { k } \operatorname { Diag } \left( \mathrm { e } ^ { - \delta \mathbf{g} / \varepsilon } \right)$\\
   $k\gets k+1$
  }
  return $\mathbf{P}$
\end{algorithm}

\subsection{Sinkhorn-Newton for Dual Problem}
For the dual problem
\begin{align}
\mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) &= \langle \mathbf { P } , \mathbf { C } \rangle - \varepsilon \mathbf { H } ( \mathbf { P } ) - \left\langle \mathbf { f } , \mathbf { P } \mathbf { 1 } _ { n } - \mathbf { a } \right\rangle - \left\langle \mathbf { g } , \mathbf { P } ^ { \mathrm { T } } \mathbf{ 1 } _ { n } - \mathbf { b } \right\rangle\\
\frac { \partial \mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) } { \partial \mathbf { P } _ { i , j } } &=0\\
\Rightarrow\hat{\mathbf { P }} &= e ^ { \mathbf { f }  / \varepsilon } e ^ { - \mathbf { C } / \varepsilon } e ^ { \mathbf { g }/ \varepsilon }\\
\Rightarrow \mathcal{L}( \hat{\mathbf { P }} , \mathbf { f } , \mathbf { g } ) &= \langle \mathbf { f } , \mathbf { a } \rangle + \langle \mathbf { g } , \mathbf { b } \rangle - \varepsilon \left\langle e ^ { \mathbf { f } / \varepsilon } , \mathbf { K } e ^ { \mathbf { g } / \varepsilon } \right\rangle
\end{align}
We only need to solve
\begin{align}
\max _ { \mathbf { f } \in \mathbb { R } ^ { n } , \mathbf { g } \in \mathbb { R } ^ { m } } \langle \mathbf { f } , \mathbf { a } \rangle + \langle \mathbf { g } , \mathbf { b } \rangle - \varepsilon \left\langle e ^ { \mathbf { f } / \varepsilon } , \mathbf { K } e ^ { \mathbf { g } / \varepsilon } \right\rangle = Q ( \mathbf { f } , \mathbf { g } )
\end{align}
(Notice that this is an approximation of the original Kantorovich dual
\begin{align}
\mathrm { L } _ { \mathbf { C } } ( \mathbf { a } , \mathbf { b } ) &= \max _ { ( \mathbf { f } , \mathbf { g } ) \in \mathbf { R } ( \mathbf { a } , \mathbf { b } ) } \langle \mathbf { f } , \mathbf { a } \rangle + \langle \mathbf { g } , \mathbf { b } \rangle\\
\mathbf { R } ( \mathbf { a } , \mathbf { b } ) &\stackrel { \mathrm { def } } { = } \left\{ ( \mathbf { f } , \mathbf { g } ) \in \mathbb { R } ^ { n } \times \mathbb { R } ^ { m } : \forall ( i , j ) \in \mathbb { Z } [ n ] \times [ m ] , \mathbf { f } \oplus \mathbf { g } \leq \mathbf { C } \right\}
\end{align}
)

We can calculate its gradient
\begin{align}
\begin{aligned} 
\left. \nabla \right| _ { \mathbf { f } } Q ( \mathbf { f } , \mathbf { g } ) & = \mathbf { a } - e ^ { \mathbf { f } / \varepsilon } \odot \left( \mathbf { K } e ^ { \mathbf { g } / \varepsilon } \right) \\
 \left. \nabla \right| _ { \mathbf { g } } Q ( \mathbf { f } , \mathbf { g } ) & = \mathbf { b } - e ^ { \mathbf { g } / \varepsilon } \odot \left( \mathbf { K } ^ { T } e ^ { \mathbf { f } / \varepsilon } \right) \end{aligned}
 \label{Q_grad}
\end{align}
and its Hessian matrix respectively.
 
Then we derive the Newton-Raphson algorithm for minimizing $ Q ( \mathbf { f } , \mathbf { g } )$ :

\begin{algorithm}[H]
  \SetAlgoNoLine
  \caption{Sinkhorn-Newton method in dual variable} 
  \KwIn{$\mathbf{a} \in \Sigma _ { n } , \mathbf{b} \in \Sigma _ { m } , \mathbf{K} \ and\ \mathbf{K} ^ { \top }$} 
  \KwOut{solution $\mathbf{P}$} 
  initializing $a ^ { 0 } \in \mathbb { R } ^ { n } , b ^ { 0 } \in \mathbb { R } ^ { m } , \operatorname { set } k = 0$\\
  \Repeat
  { some stopping criteria fulfilled }{
 $a ^ { k }\gets  \mathrm { e } ^ { - f ^ { k } / \varepsilon } \odot K \mathrm { e } ^ { - g ^ { k } / \varepsilon }$\\
 $b ^ { k } \gets  \mathrm { e } ^ { - g ^ { k } / \varepsilon } \odot K ^ { \top } \mathrm { e } ^ { - f ^ { k } / \varepsilon }$\\
 Compute updates $\delta f$ and $\delta g$ by solving
$M \left[ \begin{array} { c } { \delta f } \\ { \delta g } \end{array} \right] = \left[ \begin{array} { c } { a ^ { k } - a } \\ { b ^ { k } - b } \end{array} \right]$\\
   where the application of $M$ is given by
$M \left[ \begin{array} { c } { \delta f } \\ { \delta g } \end{array} \right] = \frac { 1 } { \varepsilon } \left[ \begin{array} { c } { a ^ { k } \odot \delta f + \mathrm { e } ^ { - f ^ { k } / \varepsilon } \odot K \left( \mathrm { e } ^ { - g ^ { k } / \varepsilon } \odot \delta g \right) } \\ { b ^ { k } \odot \delta g + e ^ { - g ^ { k } / \varepsilon } \odot K ^ { \top } \left( \mathrm { e } ^ { - f ^ { k } / \varepsilon } \odot \delta f \right) } \end{array} \right]$\\
$f ^ { k + 1 } \gets f ^ { k } + \delta f$\\
$g ^ { k + 1 } \gets  g ^ { k } + \delta g$\\
$k\gets k+1$
  }
  return $\mathbf{P}$
\end{algorithm}

Or, in \ref{Q_grad} we can set the gradient to 0 straightly
\begin{align}
\mathbf { f } ^ { ( \ell + 1 ) } &= \varepsilon \log \mathbf { a } - \varepsilon \log \left( \mathbf { K } e ^ { \mathbf { g } ^ { ( \ell ) } / \varepsilon } \right)\label{f_update}\\
\mathbf { g } ^ { ( \ell + 1 ) } &= \varepsilon \log \mathbf { b } - \varepsilon \log \left( \mathbf { K } ^ { \mathrm { T } } e ^ { \mathbf { f } ^ { ( \ell + 1 ) } / \varepsilon } \right)
\label{g_update}
\end{align}
for $\ell \ge0$. However, it's actually the same as Sinkhorn-Knopp algorithm based on $( \mathbf { u } , \mathbf { v } ) = \left( e ^ { \mathbf { f } / \varepsilon } , e ^ { \mathbf { g } / \varepsilon } \right)$.

\subsection{Log-domain Sinkhorn}
We can rewrite the above formula \ref{f_update} and \ref{g_update} as
\begin{align}
\begin{aligned} \mathbf { f } ^ { ( \ell + 1 ) } & = \operatorname { Min } _ { \varepsilon } ^ { \mathrm { row } } \left( \mathbf { S } \left( \mathbf { f } ^ { ( \ell ) } , \mathbf { g } ^ { ( \ell ) } \right) \right) - \mathbf { f } ^ { ( \ell ) } + \varepsilon \log ( \mathbf { a } ) \\ \mathbf { g } ^ { ( \ell + 1 ) } & = \operatorname { Min } _ { \varepsilon } ^ { \mathrm { col } } \left( \mathbf { S } \left( \mathbf { f } ^ { ( \ell + 1 ) } , \mathbf { g } ^ { ( \ell ) } \right) \right) - \mathbf { g } ^ { ( \ell ) } + \varepsilon \log ( \mathbf { b } ) \end{aligned}
\label{log update}
\end{align}
where $\mathbf { S } ( \mathbf { f } , \mathbf { g } ) = \left( \mathbf { C } _ { i , j } - \mathbf { f } _ { i } - \mathbf { g } _ { j } \right) _ { i , j }$ and
\begin{align}
\begin{aligned} \operatorname { Min } _ { \varepsilon } ^ { \mathrm { row } } ( \mathbf { A } ) &\stackrel { \mathrm { def } } { = } \left( \min _ { \varepsilon } \left( \mathbf { A } _ { i , j } \right) _ { j } \right) _ { i }  \in \mathbb { R } ^ { n } \\ \operatorname { Min } _ { \varepsilon } ^ { \mathrm { col } } ( \mathbf { A } ) &\stackrel { \mathrm { def} } { = }  \left( \min _ { \varepsilon } \left( \mathbf { A } _ { i , j } \right) _ { i } \right) _ { j } \in \mathbb { R } ^ { m } \end{aligned}
\end{align}
and $\min _ { \varepsilon } \mathbf { z } = - \varepsilon \log \sum _ { i } e ^ { - \mathbf { z } _ { i } / \varepsilon }$ for a vector $\mathbf{z}$.

Let's probe into this expression:

First, $\min _ { \varepsilon } \mathbf { z } $ is nothing but a differentiable approximation of $\min$ function. When we take $\epsilon \to 0$, we have $ \min _ { \varepsilon } \mathbf { z } = \min \mathbf{z}$. Besides, the above formula \ref{f_update} and \ref{g_update} are just
\begin{align}
\left( \mathbf { f } ^ { ( \ell + 1 ) } \right) _ { i } &= \min _ { \varepsilon } \left( \mathbf { C } _ { i j } - \mathbf { g } _ { j } ^ { ( \ell ) } \right) _ { j } + \varepsilon \log \mathbf { a } _ { i }\\
\left( \mathbf { g } ^ { ( \ell + 1 ) } \right) _ { j } &= \min _ { \varepsilon } \left( \mathbf { C } _ { i j } - \mathbf { f } _ { i } ^ { ( \ell ) } \right) _ { i } + \varepsilon \log \mathbf { b } _ { j }
\end{align}
where $\left( \mathbf { C } _ { i j } - \mathbf { g } _ { j } ^ { ( \ell ) } \right) _ { j }$ denotes the soft-minimum of all values of the j-th column of matrix $\left( \mathbf { C } - \mathbf { 1 } _ { n } \left( \mathbf { g } ^ { ( \ell ) } \right) ^ { \top } \right)$. If we define $ \operatorname { Min } _ { \varepsilon } ^ { \mathrm { row } } ( \mathbf { A } ) $ and $ \operatorname { Min } _ { \varepsilon } ^ { \mathrm { col } } ( \mathbf { A } ) $ as above, then we get
\begin{align}
\mathbf { f } ^ { ( \ell + 1 ) } &= \operatorname { Min } _ { \varepsilon } ^ { \mathrm { row } } \left( \mathbf { C } - \mathbf { 1 } _ { n } \mathbf { g } ^ { ( \ell ) ^ { \mathrm { T } } } \right) + \varepsilon \log \mathbf { a }\\
\mathbf { g } ^ { ( \ell + 1 ) } &= \operatorname { Min } _ { \varepsilon } ^ { \mathrm { col } } \left( \mathbf { C } - \mathbf { f } ^ { ( \ell ) } \mathbf { 1 } _ { m } ^ { \mathrm { T } } \right) + \varepsilon \log \mathbf { b }
\end{align}
After that we use a little stable trick
\begin{align}
\min _ { \varepsilon } \mathbf { z } = \underline { z } - \varepsilon \log \sum _ { i } e ^ { - \left( \mathbf { z } _ { i } - \underline { z } \right) / \varepsilon }
\end{align}
where $\underline{z} = \min \mathbf{z}$. Instead of $\underline{z}$, we use former iteration's value $\mathbf { f } ^ { ( \ell ) }$, then we get \ref{log update}.

\subsection{More on Sinkhorn}
Following [1] [2] [3] [4], the \ref{sinkhorn target} could be think of as a special case of the following convex optimization problem
\begin{align}
\min _ { \mathbf { P } } \sum _ { i , j } \mathbf { C } _ { i , j } \mathbf { P } _ { i , j } - \varepsilon \mathbf { H } ( \mathbf { P } ) + \iota _ { \{ \mathbf { a } \} } \left( \mathbf { P } \mathbf { 1 } _ { m } \right) + \iota _ { \{ \mathbf { b } \} } \left( \mathbf { P } ^ { \mathrm { T } } \mathbf { 1 } _ { n } \right)
\end{align}
where $\iota _ {  C }$ means the indicator of set $C$, which is
\begin{align}
\iota _ { \mathcal { C } } ( x ) = \left\{ \begin{array} { l l } { 0 } & { \text { if } \quad x \in \mathcal { C } } \\ { + \infty } & { \text { otherwise } } \end{array} \right.
\end{align}
Under this situation, the Sinkhorn-Knopp iteration can be extended as
\begin{align}
\mathbf { u } &\leftarrow \frac { \operatorname { Prox } _ { F } ^ { \mathrm { KL } } ( \mathbf { K } \mathbf { v } ) } { \mathbf { K } \mathbf { v } }\\
\mathbf { v }& \leftarrow \frac { \operatorname { Prox } _ { G } ^ { \mathbf { K L } } \left( \mathbf { K } ^ { \mathrm { T } } \mathbf { u } \right) } { \mathbf { K } ^ { \mathrm { T } } \mathbf { u } }
\end{align}
where the proximal operator for KL divergence is defined as
\begin{align}
\quad \operatorname { Prox } _ { F } ^ { \mathbf { K L } } ( \mathbf { u } ) = \underset { \mathbf { u } ^ { \prime } \in \mathbb { R } _ { + } ^ { N } } { \operatorname { argmin } } \mathbf { K } \mathbf { L } \left( \mathbf { u } ^ { \prime } | \mathbf { u } \right) + F \left( \mathbf { u } ^ { \prime } \right), \forall \mathbf { u } \in \mathbb { R } _ { + } ^ { N } 
\end{align}
(an extention of normal proximal operator)

\section{Numerical Result and Interpretation}
\subsection{Description of datasets}
In order to compare the performance of differnet algorithms, we have to use some classic and challenging datasets.

In general, the $i$-th datapoint can be denoted as $(x_{i},\mu_{i})$ , where $x_{i}\in \mathbb{R}^{d}$ is the position of datapoint and $\mu_{i}$ is the probability at $x_{i}$.

For convenience, we assume that datapoints are followed 2D distribution (i.e. $x_{i}\in \mathbb{R}^{2}$ ). Besides, we use the squared Euclidean distance to define the cost matrix between two datas $\{(x_{i},\mu_{i})\}_{i=1}^{m}$ and $\{(y_{j},\nu_{j})\}_{j=1}^{n}$ as following
\begin{equation}
  c_{ij}=||x_{i}-y_{j}||_{2}^{2}\quad\forall i, j
\end{equation}
We have tested our algorithms on four types of datasets
\begin{itemize}
  \item Randomly generated dataset\\
        The position are uniformly sampled from
        $[0, 1]\times[0, 1]$. The weights $\mu$ and $\nu$ are randomly sampled from $[0, 1]$ and scaled to $ \sum_{i=1}^{m}{\mu_i} = \sum_{j=1}^{n}{\nu_j} = 1 $.
  \item ellipses [\textbf{Gerber2017}]\\
  The ellipse example consists of two uniform samples (source and target data set) of size $m=n$ from the unit circle
  with normal distributed noise added with zero mean and standard deviation 0.1. The source
  data sample is then scaled in the x-Axis by 0.5 and in the y-Axis by 2, while the target
  data set is scaled in the x-Axis by 2 and in the y-Axis by 0.5.
  Besides, the weights are both normalized uniform distributions.
  \item Caffarelli [\textbf{Gerber2017}]\\
  Caffarelli’s example consists of two uniform samples (source and target data set) on $[-1, 1]\times[-1, 1]$  of size $m=n$. Any points outside the unit circle are then
discarded. Additionally, the target data sample is split along the x-Axis at 0 and shifted by
$+2$ and $-2$ for points with positive and negative x-Axis values, respectively. The weights are both normalized uniform distributions, too.
  \item DOTmark [\textbf{Schrieber2017}]\\
  In DOTmark, we always have $m=n=r^{2}$, and $(x_i)_{1\leq i\leq m} = (y_j)_{1\leq j\leq n}$ form a regular square grid of resolution $r\times r$ in $\mathbb{R}^{2}$, which are the natural position of source and target data set. The weights are the brightness distributions with normalization. Besides, DOTmark consists of 10 classes of 10 different
  images, each of which is available at the 5 different
  resolutions from $32\times32$ to $512\times512$ (in doubling steps per dimension).
\end{itemize}
\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{ellip.png}
  \captionsetup{justification=centering}
  \caption{\label{fig:ellip}$m=n=1000$, ellipse example}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{caffa.png}
  \captionsetup{justification=centering}
  \caption{\label{fig:caffa}$m=n=1000$, Caffarelli’s example}
\end{figure}
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|}
  \hline
  $\#$&Name\\
  \hline
  \hline
  1&WhiteNoise\\
  2&GRFrough\\
  3&GRFmoderate\\
  4&GRFsmooth\\
  5&LogGRF\\
  6&LogitGRF\\
  7&CauchyDensity\\
  8&Shapes\\
  9&ClassicImages\\
  10&Microscopy\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}The 10 classes in the DOTmark}
\end{table}

\subsection{Numerical result}
In this section, we will show the performance of different algorithms or solvers for the above mentioned datasets. For convenience, MOSEK and Gurobi are denoted by \textbf{M} and \textbf{G} respectively. Besides, 'prim', 'dual' and 'int' are corresponding to 'primal simplex', 'dual simplex' and 'interior point'.

In order to evaluate the performance of different algorithms and solvers, we mark down some experimental performance indicators. '\textbf{dist}' stands for the Wasserstein distance, which equals $\sum_{i}\sum_{j}c_{ij}\pi_{ij}$ for discrete optimal transport. '\textbf{time}' stands for the using time for solving OT problem. Besides, we set the fixed iterations=15000 for ADMM primal, 3000 for sinkhorn and 30 for sinkhorn-Newton in the whole numerical experiment. What's more, we also took the errors of weight into consideration via 2 indicators '\textbf{err $\mu$}' and '\textbf{err $\nu$}'.
\begin{equation}
  \begin{aligned}
  err \mu=\sum_{i=1}^{m}\left|\sum_{j=1}^{n}\pi_{ i j } - \mu_i \right|\\
  err \nu=\sum_{j=1}^{n}\left|\sum_{i=1}^{m}\pi_{ i j } - \nu_i \right|
  \end{aligned}
\end{equation}
In order to make a comprehensive comparison between different algorithms and solvers, we had tested 4 different scale, $m=n=256,512,1024,2048$ when used random, ellipse and Caffarelli dataset.
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|cccccccc|}
    \hline
    $m=n$&&M prim&M dual&M int&G prim&G dual&G int&ADMM primal&Sinkhorn\\
    \hline
    \hline
  \multirow{4}*{256}&dist&5.45e-3&7.25e-3&8.09e-3&5.94e-3&8.62e-3&5.13e-3&5.04e-3&8.64e-3\\
  ~&time&3.43e-1&3.75e-1&5.28e-1&6.79e-1&7.07e-1&8.56e-1&26.82&4.19\\  
  ~&err $\mu$&1.79e-5&2.04e-17&2.31e-12&1.34e-17&9.97e-18&4.34e-18&4.67e-5&7.01e-3\\   
  ~&err $\nu$&1.79e-5&2.45e-16&2.79e-12&5.36e-16&3.87e-16&5.05e-16&4.74e-5&8.54e-17\\
  \hline
  \multirow{4}*{512}&dist&2.90e-3&2.88e-3&3.37e-3&3.79e-3&2.91e-3&5.52e-3&3.13e-3&3.61e-3\\
  ~&time&1.39&1.70&2.93&2.83&3.40&4.27&1.03e+2&25.8\\  
  ~&err $\mu$&4.35e-5&2.67e-17&1.30e-13&1.08e-17&1.09e-15&1.21e-17&8.40e-5&8.65e-3\\   
  ~&err $\nu$&4.35e-5&1.09e-16&1.04e-13&1.72e-16&1.26e-17&7.46e-16&8.39e-5&1.20e-16\\
  \hline
  \multirow{4}*{1024}&dist&1.78e-3&1.94e-3&1.89e-3&1.91e-3&1.80e-3&1.84e-3&2.94e-3&2.79e-3\\
  ~&time&8.11&15.3&15.2&14.1&14.0&18.7&4.52e+2&1.53e+2\\  
  ~&err $\mu$&1.04e-4&1.09e-16&1.38e-12&1.01e-17&1.12e-17&1.91e-15&2.27e-4&1.07e-2\\   
  ~&err $\nu$&1.04e-4&1.91e-15&1.27e-12&6.49e-16&3.42e-16&9.43e-18&2.20e-4&1.18e-16\\
  \hline
  \multirow{4}*{2048}&dist&1.35e-3&1.36e-3&1.31e-3&1.42e-3&1.46e-3&1.49e-3&1.48e-3&1.06e-3\\
  ~&time&35.0&1.05e+2&68.7&3.45e+2&58.4&88.9&8.74e+3&1.99e+3\\  
  ~&err $\mu$&2.71e-4&2.61e-16&1.81e-12&9.41e-18&6.09e-16&4.95e-16&8.99e-4&3.93e-3\\   
  ~&err $\nu$&2.7e-4&1.79e-15&2.27e-12&1.27e-15&1.17e-17&1.13e-17&8.40e-4&1.48e-16\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}Numerical result of random dataset}
\end{table}
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|cccccccc|}
    \hline
    $m=n$&&M prim&M dual&M int&G prim&G dual&G int&ADMM primal&Sinkhorn\\
    \hline
    \hline
  \multirow{4}*{256}&dist&2.37&2.27&1.88&2.28&2.12&2.26&2.22&2.419\\
  ~&time&5.42e-1&9.80&4.10&6.57e-1&1.49&7.22e-1&37.5&20.8\\  
  ~&err $\mu$&1.76e-5&9.54e-18&1.65e-12&0&0&0&1.42e-5&6.98e-17\\   
  ~&err $\nu$&1.76e-5&9.54e-18&1.65e-12&0&0&0&1.40e-5&4.54e-16\\
  \hline
  \multirow{4}*{512}&dist&2.36&2.39&2.24&2.15&2.31&2.32&2.47&2.38\\
  ~&time&1.65&5.01&1.49&4.87&4.62&4.01&2.48e+2&72.2\\  
  ~&err $\mu$&4.05e-5&2.04e-17&2.01e-11&0&0&0&3.39e-5&1.05e-16\\   
  ~&err $\nu$&4.05e-5&1.95e-17&2.01e-11&0&0&0&3.40e-5&6.26e-16\\
  \hline
  \multirow{4}*{1024}&dist&2.24&2.11&2.21&2.24&2.18&2.27&2.25&2.39\\
  ~&time&7.89&70.4&9.73&64.5&21.9&16.6&8.41e+2&2.68e+2\\  
  ~&err $\mu$&9.39e-5&1.59e-16&5.37e-10&0&0&0&1.02e-4&1.25e-16\\   
  ~&err $\nu$&9.39e-5&1.48e-16&5.37e-10&0&0&0&1.02e-4&6.47e-16\\
  \hline
  \multirow{4}*{2048}&dist&2.38&2.23&2.28&2.27&2.22&2.37&2.31&2.27\\
  ~&time&32.6&4.37e+2&47.9&1.16e+3&8.24e+2&81.9&7.88e+3&6.52e+3\\  
  ~&err $\mu$&2.42e-4&1.29e-16&1.52e-13&0&0&0&4.67e-3&1.21e-15\\   
  ~&err $\nu$&2.42e-4&1.31e-16&1.40e-13&0&0&0&4.65e-3&1.07e-15\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}Numerical result of ellipse example}
\end{table}
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|cccccccc|}
    \hline
    $m=n$&&M prim&M dual&M int&G prim&G dual&G int&ADMM primal&Sinkhorn\\
    \hline
    \hline
  \multirow{4}*{256}&dist&4.06&3.91&4.03&4.08&3.86&4.15&4.02&4.24\\
  ~&time&2.82e-1&8.80&6.54e-1&6.80e-1&1.96&7.63e-1&55.2&12.4\\  
  ~&err $\mu$&1.67e-5&2.78e-17&1.98e-12&0&0&0&4.07e-6&6.46e-17\\   
  ~&err $\nu$&1.67e-5&2.78e-17&1.98e-12&0&0&0&4.03e-6&4.59e-16\\
  \hline
  \multirow{4}*{512}&dist&3.95&3.97&4.05&4.10&3.97&3.96&3.99&4.21\\
  ~&time&1.88&5.37&3.60&3.50&3.61&4.63&4.00e+2&50.3\\  
  ~&err $\mu$&4.14e-5&7.55e-17&5.79e-12&0&0&0&4.03e-5&9.84e-17\\   
  ~&err $\nu$&4.14e-5&7.59e-17&5.79e-12&0&0&0&4.08e-5&6.09e-16\\
  \hline
  \multirow{4}*{1024}&dist&3.99&4.04&4.09&4.00&4.00&3.95&3.97&4.35\\
  ~&time&6.01&49.9&14.8&29.9&18.7&22.8&1.52e+3&1.72e+2\\  
  ~&err $\mu$&9.80e-5&1.40e-16&8.04e-12&0&0&0&4.42e-4&1.33e-16\\   
  ~&err $\nu$&9.80e-5&1.40e-16&8.12e-12&0&0&0&4.43e-4&6.29e-16\\
  \hline
  \multirow{4}*{2048}&dist&3.97&3.99&4.06&3.99&3.97&4.00&4.04&4.13\\
  ~&time&27.9&4.93e+2&58.0&7.47e+2&1.02e+2&1.02e+1&1.06e+4&8.44e+3\\  
  ~&err $\mu$&2.54e-4&3.95e-16&2.53e-13&0&0&0&8.60e-3&1.83e-16\\   
  ~&err $\nu$&2.54e-4&3.96e-16&2.49e-13&0&0&0&8.60e-3&9.74e-16\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}Numerical result of Caffarelli's example}
\end{table}
Due to the limited time, we
only tested a randomly chosen pair of images from each class with size $32\times32$, whose corresponding cost matirx is $1024\times1024$.
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|cccccccc|}
  \hline
  $\#$&&M prim&M dual&M int&G prim&G dual&G int&ADMM primal&Sinkhorn\\
  \hline
  \hline
  \multirow{4}*{1}&dist&6.93e-4&6.93e-4&6.93e-4&6.93e-4&6.93e-4&6.93e-4&6.92e-4&6.62e-4\\
  ~&time&9.50&10.0&11.9&15.6&11.3&1.73e+2&1.46e+3&1.49e+2\\
  ~&err $\mu$&1.02e-4&1.59e-16&3.39e-12&0&0&0&1.01e-4&7.90e-3\\
  ~&err $\nu$&1.02e-4&1.19e-9&1.20e-9&1.19e-9&1.19e-9&1.19e-9&1.01e-4&6.63e-17\\
  \hline
  \multirow{4}*{2}&dist&1.44e-3&1.44e-3&1.44e-3&1.44e-3&1.44e-3&1.44e-3&1.44e-3&1.27e-3\\
  ~&time&7.34&15.7&10.3&14.4&14.4&26.5&1.59e+3&1.51e+2\\
  ~&err $\mu$&9.91e-5&1.10e-16&6.47e-13&0&0&2.58e-9&1.72e-4&1.15e-2\\
  ~&err $\nu$&9.91e-5&2.58e-9&2.58e-9&2.58e-9&2.58e-9&0&1.68e-4&1.29e-16\\
  \hline
  \multirow{4}*{3}&dist&3.98e-3&3.98e-3&3.98e-3&3.98e-3&3.98e-3&3.98e-3&3.98e-3&3.99e-3\\
  ~&time&5.55&20.4&10.5&13.64&13.2&18.49&1.48e+3&1.52e+2\\
  ~&err $\mu$&9.84e-5&1.89e-16&7.38e-14&0&5.70e-10&0&1.16e-4&1.12e-3\\
  ~&err $\nu$&9.84e-5&5.70e-10&5.70e-10&5.70e-10&0&0&1.63e-4&1.82e-16\\
  \hline
  \multirow{4}*{4}&dist&2.09e-2&2.09e-2&2.09e-2&2.09e-2&2.09e-2&2.09e-2&2.09e-2&2.20e-2\\
  ~&time&5.56&31.8&9.55&13.9&15.9&22.1&1.44e+3&1.25e+2\\
  ~&err $\mu$&9.78e-5&1.67e-16&9.80e-12&0&0&0&1.65e-4&1.23e-9\\
  ~&err $\nu$&9.78e-5&9.89e-8&1.24e-9&1.23e-9&1.23e-9&1.23e-9&1.63e-4&3.40e-16\\
  \hline
  \multirow{4}*{5}&dist&1.87e-2&1.87e-2&1.87e-2&1.87e-2&1.87e-2&1.87e-2&1.87e-2&1.97e-2\\
  ~&time&5.99&20.4&10.3&14.2&16.8&29.4&1.54e+3&1.32e+2\\  
  ~&err $\mu$&1.00e-4&1.55e-16&2.93e-12&0&0&0&1.16e-4&1.70e-9\\   
  ~&err $\nu$&1.00e-4&1.59e-9&1.59e-9&1.59e-9&8.56e-10&1.59e-9&1.35e-4&3.07e-16\\
  \hline
  \multirow{4}*{6}&dist&1.65e-2&1.65e-2&1.65e-2&1.65e-2&1.65e-2&1.65e-2&1.65e-2&1.74e-2\\
   ~&time&6.43&18.1&13.7&13.4&16.8&19.1&1.42e+3&1.31e+2\\   
   ~&err $\mu$&1.00e-4&1.80e-16&4.20e-10&0&0&0&1.17e-4&4.11e-8\\   
   ~&err $\nu$&1.00e-4&8.56e-10&1.28e-9&1.16e-9&8.56e-10&8.56e-10&1.20e-4&2.74e-16\\
  \hline
  \multirow{4}*{7}&dist&1.71e-2&1.71e-2&1.71e-2&1.71e-2&1.71e-2&1.71e&1.71e-2&1.80e-2\\
    ~&time&8.66&29.7&12.5&13.4&18.0&26.4&1.54e+3&91.3\\   
    ~&err $\mu$&1.09e-4&1.19e-16&4.09e-10&0&0&0&5.64e-4&1.16e-9\\   
    ~&err $\nu$&1.09e-4&1.16e-9&1.57e-9&1.16e-9&1.16e-9&1.16e-9&4.35e-4&2.89e-16\\
  \hline
  \multirow{4}*{8}&dist&2.38e-2&2.38e-2&2.38e-2&2.38e-2&2.38e-2&2.38e-2&2.38e-2&2.50e-2\\
    ~&time&5.17&6.84&6.33&13.3&12.2&12.2&1.56e+3&1.41e+2\\   
    ~&err $\mu$&6.52e-5&1.17e-16&4.72e-11&0&0&0&8.20e-4&2.24e-8\\   
    ~&err $\nu$&6.53e-5&2.24e-8&2.25e-8&2.24e-8&2.24e-8&2.24e-8&2.94e-4&4.48e-16\\
  \hline
  \multirow{4}*{9}&dist&6.12e-3&6.12e-3&6.12e-3&6.12e-3&6.12e-3&6.12e-3&6.13e-3&6.20e-3\\
    ~&time&5.71&17.56&12.9&15.1&13.2&18.4&1.62e+3&1.51e+2\\   
    ~&err $\mu$&9.90e-5&1.61e-16&9.08e-13&0&2.18e-11&0&1.16e-4&2.18e-11\\ 
    ~&err $\nu$&9.90e-5&2.18e-11&2.26e-11&2.18e-11&0&2.18e-11&1.21e-4&5.55e-16\\
  \hline
  \multirow{4}*{10}&dist&1.06e-2&1.06e-2&1.06e-2&1.06e-2&1.06e-2&1.06e-2&1.06e-2&1.09e-2\\
    ~&time&4.20&7.32&6.00&12.8&13.9&20.9&1.71e+3&1.51e+2\\   
    ~&err $\mu$&7.01e-5&1.16e-16&2.40e-11&0&0&5.94e-9&1.20e-4&1.23e-6\\   
    ~&err $\nu$&7.01e-5&5.94e-9&5.95e-9&5.94e-9&5.94e-9&0&1.76e-4&2.24e-16\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}Numerical result of 10 classes in the DOTmark}
\end{table}
For solvers, it seems that Gurobi may always reach the truly optimal solution, while MOSEK sometimes fails and result in large errors of weight. For MOSEK, the primal simplex method are faster than the interior method and the dual simplex method, but larger errors of weight. For Gurobi, simplex methods are generally faster than interior point methods, because an simplex methods are specified for linear programs. The bad proformance of the dual simplex may result from the huge amount of constraints.
Gurobi being generally better than MOSEK because of defect of the Python interface of MOSEK.

From the overall numerical results,  we found that all the algorithms converge, but there are still some difference. As a stable and universal algorithm,the performance of ADMM primal is  not completely capable of different problems. ADMM primal needs thousands of iterations to reach optimal value, but errors of weight needs more iterations to reach 1e-4 or fewer. Thus, the using time of ADMM primal is the worst one.
\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{loss1.png}
  \captionsetup{justification=centering}
  \caption{\label{fig:loss}$m=n=1000$, solving Caffarelli’s example via ADMM primal}
\end{figure}

The implementation and thinking of Sinkhorn algorithm are nothing difficult. The number of hyperparameter is only one, $\epsilon$. From the above mentioned theoratical introduction of Sinkhorn, $\epsilon$ stands for the weight of entropy regularization. If $\epsilon$ is extremely large, the entropy term will be the dominant of the problem, which will mislead the original problem to a completely different problem. Thus, the optimal solution of this problem will larger than true optimal value. On the other hand, If $\epsilon$ is extremely small, the entropy term can be ignored and makes no difference in this problem. In particular experiment, when using extremely small $\epsilon$, the algorithm will be unstable and always overflow due to the defect of float point.

All the codes are implemented in Python. 
\subsection{A discussion of Sinkhorn-Newton algorithm}
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|ccc|}
    \hline
    $m=n$&&random&ellipse&Caffarelli\\
    \hline
    \hline
  \multirow{4}*{256}&dist&9.70e-3&2.59&4.53\\
  ~&time&14.4&23.9&11.0\\  
  ~&err $\mu$&8.82e-15&3.01e-15&2.68e-16\\   
  ~&err $\nu$&1.47e-14&3.53e-15&3.21e-16\\
  \hline
  \multirow{4}*{512}&dist&6.22e-3&2.48&4.23\\
  ~&time&67.7&95.4&80.5\\  
  ~&err $\mu$&8.75e-16&6.15e-16&1.43e-15\\   
  ~&err $\nu$&1.24e-15&7.05e-16&1.53e-15\\
  \hline
  \multirow{4}*{1024}&dist&5.4e-3&2.49&4.28\\
  ~&time&1.69e+2&2.83e+2&2.52e+2\\  
  ~&err $\mu$&1.72e-14&2.02e-16&1.25e-16\\   
  ~&err $\nu$&2.15e-14&5.96e-16&6.28e-16\\
  \hline
  \multirow{4}*{2048}&dist&4.36e-3&2.50&4.28\\
  ~&time&1.22e+3&1.18e+3&1.23e+3\\  
  ~&err $\mu$&1.88e-14&2.23e-16&1.90e-16\\   
  ~&err $\nu$&2.39e-14&8.64e-16&8.59e-16\\
  \hline
  \end{tabular}
  \caption{\label{tab:table1}Numerical result of Sinkhorn-Newtom algorithm}
\end{table}
Comparing with sinkhorn algorithm, we can easily find that the using time of sinkhorn-Newton are longer than sinkhorn at small-scale datasets. Because, sinkhorn-Newton algorithm has an expensive step to solve a linear equation. However, when solving large-scale datasets, the efficieny of sinkhorn-Newton is better than sinkhorn due to fewer iterations. For sinkhorn algorithm, it usually needs hundreds of iterations in order to decrease err $\mu$ , while sinkhorn-Newton algorithm only needs dozens of iterations. Besides, sinkhorn-Newton algorithm has great performance of err $\mu$ and err $\nu$, even better than Gurobi and MOSEK. 

However, sinkhorn-Newton algorithm isn't a stable and universal algorithm. The value of hyperparameter $\epsilon$ is extremely sensitive towards different datasets. From the simplification of algorithm, we can find that $\epsilon$ only makes a difference at the initial step to smooth cost matrix $c$. In this way, if $\epsilon$ is extremely large, the cost matrix will close to zero matrix, which smooths over the characteristics of different cost matrix and results in bigger distance between two datas than Wasserstein distance. On the other hand, if $\epsilon$ is lower than specific critical value, the singularity of cost matrix will be strengthened via exponential function, which results in the Instability of solving linear equation and finally overflowing.
\subsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.
\end{large}
\section*{References}
\medskip
\small

[1] Karlsson, J., \& Ringh, A. (2017). Generalized Sinkhorn iterations for regularizing inverse problems using optimal mass transport. SIAM Journal on Imaging Sciences, 10(4), 1935-1962.

[2] Peyré, G. (2015). Entropic approximation of Wasserstein gradient flows. SIAM Journal on Imaging Sciences, 8(4), 2323-2351.

[3] Frogner, C., Zhang, C., Mobahi, H., Araya, M., \& Poggio, T. A. (2015). Learning with a Wasserstein loss. In Advances in Neural Information Processing Systems (pp. 2053-2061).

[4] Chizat, L., Peyré, G., Schmitzer, B., \& Vialard, F. X. (2016). Scaling algorithms for unbalanced transport problems. arXiv preprint arXiv:1607.05816.
 
 [5] L. Ambrosio. Lecture notes on optimal transport problems. In Mathematical Aspects of Evolving Interfaces, volume 1812 of Lecture Notes in Mathematics, pages 1–52. Springer, 2003
\end{document}