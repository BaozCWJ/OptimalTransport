\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Large-scale Optimal Transport}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.


\author{
  Weijie Chen\thanks{Pre-admission 2019 PKU AAIS} \\
  School of Physics\\
  Peking University\\
  1500011335 \\
  \texttt{1500011335@pku.edu.cn} \\
  \And
  Dinghuai Zhang \\
  School of Mathematics\\
  Peking University\\
  1600013525\\
  \texttt{1600013525@pku.edu.cn} \\
}



\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  
\end{abstract}

\section{Introduction to Optimal Transport}

\section{Problem Statement}
The standard formulation of optimal transport are derived from couplings. [\textbf{Villani2009}] That is, let $ \left(\mathcal{X}, \mu \right)$ and $\left(\mathcal{Y}, \nu \right)$  be two probability spaces, and a probability distribution $\pi$ on $ \mathcal{X} \times \mathcal{Y} $ is called \emph{coupling} if $ proj_{\mathcal{X}} (\pi) = \mu $ and $ proj_{\mathcal{Y}} (\pi)= \nu $. An optimal transport between $ \left(\mathcal{X}, \mu \right)  $ and $ \left(\mathcal{Y}, \nu \right) $, or an optimal coupling, is a coupling minimize
\begin{equation}
\int_{ \mathcal{X} \times \mathcal{Y} } c ( x, y)  d \pi ( x, y ) 
\end{equation}

Optimal transport problems can be categorized according to the discreteness of $\mu$ and $\nu$. In this report, we only consider discrete optimal tranport problems, where the two distributions are distributions of finite weighted points.

A discrete optimal transport problem can be formulated into a linear program as
\begin{equation} \label{Eq:StdLP}
\begin{aligned}
\min_{\pi} & \sum_{i=1}^{m}\sum_{j=1}^{n} c_{ i j } \pi_{ i j }\\
s.t. & \sum_{j=1}^{n}\pi_{ i j } = \mu_i, \forall i\\
& \sum_{i=1}^{n}\pi_{ i j } = \nu_j, \forall j \\
& \pi_{ij} \geq 0,
\end{aligned}
\end{equation}
where $c$ stands for the cost and $s$ for the transportation plan, while $\mu$ and $\nu$ are restrictions. Note that we always suppose $ c \geq 0 $, $ \mu \geq 0 $, $ \nu \geq 0 $ and $ \sum_{i=1}^{m}{\mu_i} = \sum_{j=1}^{n}{\nu_j} = 1 $ implicitly. From realistic background, $c$ is always valued the squared Euclidean distanced or some other norms. Note that there are $ m n $ variables in this formulation, and this leads to intensive computation.

\section{Algorithms}
\subsection{ADMM for Primal Problem}
\subsection{ADMM for Dual Problem}
\subsection{Sinkhorn Method with Entropy Regularization}
The discrete entropy of a coupling matrix is defined as
\begin{equation}
\mathbf { H } ( \mathbf { P } ) \stackrel { \mathrm { def } } { = } - \sum _ { i , j } \mathbf { P } _ { i , j } \left( \log \left( \mathbf { P } _ { i , j } \right) - 1 \right)
\end{equation}
The function $\mathbf{H}$ is strongly concave.

The idea of the entropic regularization of optimal transport is to use $-\mathbf{H}$ as a regularizing function to obtain approximate solutions to the original transport problem:
\begin{equation}
\mathrm { L } _ { \mathrm { C } } ^ { \varepsilon } ( \mathbf { a } , \mathbf { b } ) \stackrel { \mathrm { def } } { = } \min _ { \mathbf { P } \in \mathbf { U } ( \mathbf { a } , \mathbf { b } ) } \langle \mathbf { P } , \mathbf { C } \rangle - \varepsilon \mathbf { H } ( \mathbf { P } )
\label{sinkhorn target}
\end{equation}
(Actually, this can be interpreted as $\text{KL}(\mathbf{P}||\mathbf{K})$)

One can show that the solution to \ref{sinkhorn target} has the form of 
\begin{equation}
\mathbf { P } _ { i , j } = \mathbf { u } _ { i } \mathbf { K } _ { i , j } \mathbf { v } _ { j }
\end{equation}
where $\mathbf { K } _ { i , j } = e^{-\mathbf{C}_{i,j}/\epsilon}$ by calculating the KKT condition:
Introducing two dual variables $\mathbf { f } \in \mathbb { R } ^ { n } , \mathbf { g } \in \mathbb { R } ^ { n }$ and calculate the lagrangian:
\begin{equation}
\mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) = \langle \mathbf { P } , \mathbf { C } \rangle - \varepsilon \mathbf { H } ( \mathbf { P } ) - \left\langle \mathbf { f } , \mathbf { P } \mathbf { 1 } _ { n } - \mathbf { a } \right\rangle - \left\langle \mathbf { g } , \mathbf { P } ^ { \mathrm { T } } \mathbf{ 1 } _ { n } - \mathbf { b } \right\rangle
\end{equation}
take first order gradient and we get
\begin{align}
\frac { \partial \mathcal { L } ( \mathbf { P } , \mathbf { f } , \mathbf { g } ) } { \partial \mathbf { P } _ { i , j } } &= \mathbf { C } _ { i , j } + \varepsilon \log \left( \mathbf { P } _ { i , j } \right) - \mathbf { f } _ { i } - \mathbf { g } _ { j } = 0\\
\Rightarrow\mathbf { P } _ { i , j } &= e ^ { \mathbf { f } _ { i } / \varepsilon } e ^ { - \mathbf { C } _ { i , j } / \varepsilon } e ^ { \mathbf { g } _ { j } / \varepsilon }
\end{align}
Based on the constrain that:
\begin{align}
\operatorname { diag } ( \mathbf { u } ) \mathbf { K } \operatorname { diag } ( \mathbf { v } ) \mathbf { 1 } _ { m } &= \mathbf { a }\\
\operatorname { diag } ( \mathbf { v } ) \mathbf { K } ^ { \top } \operatorname { diag } ( \mathbf { u } ) \mathbf { 1 } _ { n } &= \mathbf { b }
\end{align}
or :
\begin{align}
\mathbf { u } \odot ( \mathbf { K } \mathbf { v } ) = \mathbf { a } \quad \text { and } \quad \mathbf { v } \odot \left( \mathbf { K } ^ { \mathrm { T } } \mathbf { u } \right) = \mathbf { b }
\end{align}
(where $\odot$ means entry-wise multiplication of vectors) we can develop our algorithm as iteratively updating $\mathbf { u }$ and $\mathbf { v }$:
\begin{align}
\mathbf { u } ^ { ( \ell + 1 ) }  { = } \frac { \mathbf { a } } { \mathbf { K } \mathbf { v } ^ { ( \ell ) } } \text { and } \mathbf { v } ^ { ( \ell + 1 ) } { = } \frac { \mathbf { b } } { \mathbf { K } ^ { \mathrm { T } } \mathbf { u } ^ { ( \ell + 1 ) } }
\end{align}
with $\mathbf { v } ^ { ( 0 ) } = \mathbf { 1 } _ { m }$ and $\mathbf { K } _ { i , j } = e^{-\mathbf{C}_{i,j}/\epsilon}$.

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level
heading for the references. Any choice of citation style is acceptable
as long as you are consistent. It is permissible to reduce the font
size to \verb+small+ (9 point) when listing the references. {\bf
  Remember that you can go over 8 pages as long as the subsequent ones contain
  \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}